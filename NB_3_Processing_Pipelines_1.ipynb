{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **<span style=\"color: #2e86de;\">S5P TROPOMI Data Access Hub API Test - Pipeline 1</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **<span style=\"color: #27ae60;\">Load Toolkit Functions</span>**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_1352\\3789936564.py:3: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ pandas is already installed.\n",
      "‚¨ÜÔ∏è Successfully upgraded pandas\n",
      "‚úÖ geopandas is already installed.\n",
      "‚¨ÜÔ∏è Successfully upgraded geopandas\n",
      "‚úÖ shapely is already installed.\n",
      "‚¨ÜÔ∏è Successfully upgraded shapely\n",
      "‚úÖ Successfully converted E:/SentinelNO2-Detection/NB_2_Sentinal_Tools.ipynb to E:/SentinelNO2-Detection/NB_2_Sentinal_Tools.py\n",
      "‚úÖ pandas is already installed.\n",
      "‚¨ÜÔ∏è Successfully upgraded pandas\n",
      "‚úÖ geopandas is already installed.\n",
      "‚¨ÜÔ∏è Successfully upgraded geopandas\n",
      "‚úÖ netCDF4 is already installed.\n",
      "‚¨ÜÔ∏è Successfully upgraded netCDF4\n",
      "‚úÖ numpy is already installed.\n",
      "‚¨ÜÔ∏è Successfully upgraded numpy\n",
      "‚úÖ matplotlib is already installed.\n",
      "‚¨ÜÔ∏è Successfully upgraded matplotlib\n",
      "‚úÖ requests is already installed.\n",
      "‚¨ÜÔ∏è Successfully upgraded requests\n",
      "‚úÖ xmltodict is already installed.\n",
      "‚¨ÜÔ∏è Successfully upgraded xmltodict\n",
      "‚úÖ shapely is already installed.\n",
      "‚¨ÜÔ∏è Successfully upgraded shapely\n",
      "üåü All libraries are imported successfully!\n",
      "‚úÖ All libraries are checked, installed, upgraded, and imported successfully!\n",
      "‚úÖ All libraries are imported successfully!\n",
      "‚úÖ All specified libraries are checked, installed, upgraded, and imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import pkg_resources\n",
    "# Path to the Jupyter notebook\n",
    "notebook_path = \"E:/SentinelNO2-Detection/NB_2_Sentinal_Tools.ipynb\"\n",
    "script_path = \"E:/SentinelNO2-Detection/NB_2_Sentinal_Tools.py\"\n",
    "# Convert Jupyter notebook to Python script\n",
    "def convert_notebook_to_script(notebook, script):\n",
    "    try:\n",
    "        result = subprocess.run([\"jupyter\", \"nbconvert\", \"--to\", \"script\", notebook], capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(f\"‚úÖ Successfully converted {notebook} to {script}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Error converting {notebook}: {result.stderr}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚ùå Error converting {notebook}: {e}\")\n",
    "# List of libraries to check/install/upgrade\n",
    "libraries = [\n",
    "    'pandas', \n",
    "    'geopandas', \n",
    "    'shapely'\n",
    "]\n",
    "# Function to install a package with a progress bar\n",
    "def install_package(package):\n",
    "    try:\n",
    "        result = subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", package], capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(f\"‚úÖ Successfully installed {package}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Error installing {package}: {result.stderr}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚ùå Error installing {package}: {e}\")\n",
    "# Function to upgrade a package with a progress bar\n",
    "def upgrade_package(package):\n",
    "    try:\n",
    "        result = subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", package], capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(f\"‚¨ÜÔ∏è Successfully upgraded {package}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Error upgrading {package}: {result.stderr}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚ùå Error upgrading {package}: {e}\")\n",
    "# Function to check if a package is installed and install/upgrade it\n",
    "def check_and_install(library):\n",
    "    try:\n",
    "        pkg_resources.get_distribution(library)\n",
    "        print(f\"‚úÖ {library} is already installed.\")\n",
    "        upgrade_package(library)\n",
    "    except pkg_resources.DistributionNotFound:\n",
    "        print(f\"üì¶ {library} is not installed. Installing now...\")\n",
    "        install_package(library)\n",
    "# Function to log the status\n",
    "def log_status(library, status):\n",
    "    with open('library_installation_log.txt', 'a') as log_file:\n",
    "        log_file.write(f\"{library}: {status}\\n\")\n",
    "# Enhanced check and install/upgrade packages with logging\n",
    "for library in libraries:\n",
    "    check_and_install(library)\n",
    "    log_status(library, \"Checked and processed\")\n",
    "# Convert the Jupyter notebook to a Python script\n",
    "convert_notebook_to_script(notebook_path, script_path)\n",
    "# Import all the necessary libraries\n",
    "def import_libraries():\n",
    "    globals().update(locals())\n",
    "    import pandas as pd\n",
    "    import geopandas as gpd\n",
    "    from shapely import wkt\n",
    "    from NB_2_Sentinal_Tools import sentinel_api_query, date_from_week, add_days, geometry_to_wkt, filter_swath_set, get_place_boundingbox, show_colormap\n",
    "# Try to import all libraries and handle any potential import errors\n",
    "try:\n",
    "    import_libraries()\n",
    "    print(\"‚úÖ All libraries are imported successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Error importing libraries: {e}\")\n",
    "    log_status('import', f\"Error: {e}\")\n",
    "print(\"‚úÖ All specified libraries are checked, installed, upgraded, and imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **<span style=\"color: #8e44ad;\">Obtain Target Country GeoDataFrame</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project setup completed successfully! Check setup_log.log for details.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import requests\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    filename=\"setup_log.log\",\n",
    "    filemode=\"w\",\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "# Define the base directory path\n",
    "base_path = Path(\"E:/SentinelNO2-Detection/Geo_Data_Downloading\")\n",
    "# Project structure and dependencies\n",
    "folders = [\"data\"]\n",
    "files = {\n",
    "    \"requirements.txt\": \"flask\\nrequests\\n\",\n",
    "}\n",
    "# Function to create folders and files\n",
    "def create_project_structure():\n",
    "    try:\n",
    "        # Ensure the base directory exists\n",
    "        base_path.mkdir(parents=True, exist_ok=True)       \n",
    "        # Create folders\n",
    "        for folder in folders:\n",
    "            folder_path = base_path / folder\n",
    "            folder_path.mkdir(exist_ok=True)\n",
    "            logging.info(f\"Created folder: {folder_path}\")\n",
    "        # Create and write files\n",
    "        for file, content in files.items():\n",
    "            file_path = base_path / file\n",
    "            with open(file_path, \"w\") as f:\n",
    "                f.write(content)\n",
    "            logging.info(f\"Created file: {file_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error creating project structure: {e}\")\n",
    "# Function to check and install Node.js and npm if not present\n",
    "def check_and_install_node():\n",
    "    try:\n",
    "        node_installed = subprocess.run([\"node\", \"-v\"], capture_output=True)\n",
    "        npm_installed = subprocess.run([\"npm\", \"-v\"], capture_output=True)\n",
    "        if node_installed.returncode != 0 or npm_installed.returncode != 0:\n",
    "            logging.info(\"Node.js or npm not detected. Downloading installer.\")\n",
    "            # Download Node.js installer (Windows example)\n",
    "            installer_url = \"https://nodejs.org/dist/v18.12.1/node-v18.12.1-x64.msi\"\n",
    "            installer_path = base_path / \"node_installer.msi\"\n",
    "            response = requests.get(installer_url)\n",
    "            with open(installer_path, \"wb\") as file:\n",
    "                file.write(response.content)\n",
    "            subprocess.run([\"msiexec\", \"/i\", str(installer_path), \"/quiet\"])\n",
    "            logging.info(\"Node.js and npm installed successfully.\")\n",
    "        else:\n",
    "            logging.info(\"Node.js and npm are already installed.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error checking/installing Node.js: {e}\")\n",
    "# Function to install Python packages from requirements.txt\n",
    "def install_python_packages():\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"])\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", str(base_path / \"requirements.txt\")])\n",
    "        logging.info(\"Python packages installed successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error installing Python packages: {e}\")\n",
    "# Function to upgrade outdated Python packages\n",
    "def upgrade_packages():\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"])\n",
    "        outdated_packages = subprocess.check_output([sys.executable, \"-m\", \"pip\", \"list\", \"--outdated\", \"--format=freeze\"]).decode().splitlines()\n",
    "        for package in outdated_packages:\n",
    "            package_name = package.split(\"==\")[0]\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", package_name])\n",
    "            logging.info(f\"Upgraded package: {package_name}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error upgrading packages: {e}\")\n",
    "# Function to update requirements.txt with installed packages not already listed\n",
    "def update_requirements_file():\n",
    "    try:\n",
    "        # Read existing requirements\n",
    "        req_file_path = base_path / \"requirements.txt\"\n",
    "        with open(req_file_path, \"r\") as f:\n",
    "            existing_packages = set(line.strip() for line in f if line.strip())\n",
    "        # Get all installed packages\n",
    "        installed_packages = subprocess.check_output([sys.executable, \"-m\", \"pip\", \"freeze\"]).decode().splitlines()\n",
    "        installed_packages_set = {pkg.split(\"==\")[0] for pkg in installed_packages}\n",
    "        # Identify new packages not in requirements.txt\n",
    "        new_packages = installed_packages_set - existing_packages\n",
    "        # Append new packages to requirements.txt\n",
    "        with open(req_file_path, \"a\") as f:\n",
    "            for pkg in installed_packages:\n",
    "                pkg_name = pkg.split(\"==\")[0]\n",
    "                if pkg_name in new_packages:\n",
    "                    f.write(pkg + \"\\n\")\n",
    "                    logging.info(f\"Added {pkg} to requirements.txt\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error updating requirements.txt: {e}\")\n",
    "# Main setup function\n",
    "def main_setup():\n",
    "    logging.info(\"Starting project setup...\")\n",
    "    create_project_structure()\n",
    "    check_and_install_node()\n",
    "    install_python_packages()\n",
    "    upgrade_packages()\n",
    "    update_requirements_file()\n",
    "    logging.info(\"Project setup completed successfully.\")\n",
    "    print(\"Project setup completed successfully! Check setup_log.log for details.\")\n",
    "# Call main setup function directly\n",
    "main_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required files have been successfully downloaded.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    filename=\"download_log.log\",\n",
    "    filemode=\"w\",\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "# Create a separate error log\n",
    "error_log_file = \"error_log.log\"\n",
    "logging.getLogger().addHandler(logging.FileHandler(error_log_file))\n",
    "# URLs to download\n",
    "urls = {\n",
    "    \"osm_pbf\": [\n",
    "        \"https://download.geofabrik.de/asia/india/central-zone-latest.osm.pbf\",\n",
    "        \"https://download.geofabrik.de/asia/india/eastern-zone-latest.osm.pbf\",\n",
    "        \"https://download.geofabrik.de/asia/india/north-eastern-zone-latest.osm.pbf\",\n",
    "        \"https://download.geofabrik.de/asia/india/northern-zone-latest.osm.pbf\",\n",
    "        \"https://download.geofabrik.de/asia/india/southern-zone-latest.osm.pbf\",\n",
    "        \"https://download.geofabrik.de/asia/india/western-zone-latest.osm.pbf\"\n",
    "    ],\n",
    "    \"shp_zip\": [\n",
    "        \"https://download.geofabrik.de/asia/india/central-zone-latest-free.shp.zip\",\n",
    "        \"https://download.geofabrik.de/asia/india/eastern-zone-latest-free.shp.zip\",\n",
    "        \"https://download.geofabrik.de/asia/india/north-eastern-zone-latest-free.shp.zip\",\n",
    "        \"https://download.geofabrik.de/asia/india/northern-zone-latest-free.shp.zip\",\n",
    "        \"https://download.geofabrik.de/asia/india/southern-zone-latest-free.shp.zip\",\n",
    "        \"https://download.geofabrik.de/asia/india/western-zone-latest-free.shp.zip\"\n",
    "    ],\n",
    "    \"osm_bz2\": [\n",
    "        \"https://download.geofabrik.de/asia/india/central-zone-latest.osm.bz2\",\n",
    "        \"https://download.geofabrik.de/asia/india/eastern-zone-latest.osm.bz2\",\n",
    "        \"https://download.geofabrik.de/asia/india/north-eastern-zone-latest.osm.bz2\",\n",
    "        \"https://download.geofabrik.de/asia/india/northern-zone-latest.osm.bz2\",\n",
    "        \"https://download.geofabrik.de/asia/india/southern-zone-latest.osm.bz2\",\n",
    "        \"https://download.geofabrik.de/asia/india/western-zone-latest.osm.bz2\"\n",
    "    ]\n",
    "}\n",
    "# Download path configuration\n",
    "download_directory = \"E:/SentinelNO2-Detection/Geo_Data_Downloading/data\"\n",
    "os.makedirs(download_directory, exist_ok=True)\n",
    "# Setup Chrome WebDriver with configured download path\n",
    "def setup_browser(download_directory):\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--start-maximized\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    prefs = {\"download.default_directory\": download_directory}\n",
    "    chrome_options.add_experimental_option(\"prefs\", prefs)\n",
    "    # Initialize WebDriver with ChromeDriverManager\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    return driver\n",
    "# Download file with progress indicator\n",
    "def download_with_progress(url, download_directory):\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    file_path = os.path.join(download_directory, filename)\n",
    "    if os.path.exists(file_path):\n",
    "        logging.info(f\"{filename} already exists. Skipping download.\")\n",
    "        return True\n",
    "    try:\n",
    "        # Start the download process\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()  # Raise an error for bad responses\n",
    "        file_size = int(response.headers.get('Content-Length', 0))\n",
    "        logging.info(f\"Downloading {filename} ({file_size / (1024 ** 2):.2f} MB)...\")\n",
    "        # Display progress\n",
    "        with open(file_path, \"wb\") as file, tqdm(\n",
    "                desc=filename,\n",
    "                total=file_size,\n",
    "                unit=\"B\",\n",
    "                unit_scale=True,\n",
    "                unit_divisor=1024,\n",
    "        ) as progress:\n",
    "            for chunk in response.iter_content(chunk_size=4096):\n",
    "                if chunk:\n",
    "                    file.write(chunk)\n",
    "                    progress.update(len(chunk))\n",
    "        logging.info(f\"{filename} downloaded successfully.\")\n",
    "        update_progress_file(filename)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error downloading {filename}: {e}\")\n",
    "        return False\n",
    "# Update the progress file with downloaded filenames\n",
    "def update_progress_file(filename):\n",
    "    with open(\"progress.txt\", \"a\") as progress_file:\n",
    "        progress_file.write(f\"{filename}\\n\")\n",
    "# Verify all required files are downloaded\n",
    "def verify_downloads():\n",
    "    downloaded_files = set()\n",
    "    if os.path.exists(\"progress.txt\"):\n",
    "        with open(\"progress.txt\", \"r\") as progress_file:\n",
    "            downloaded_files = {line.strip() for line in progress_file}\n",
    "    all_required_files = {url.split(\"/\")[-1] for category in urls.values() for url in category}\n",
    "    missing_files = all_required_files - downloaded_files\n",
    "    if missing_files:\n",
    "        logging.warning(\"Missing files: \" + \", \".join(missing_files))\n",
    "        print(\"Missing files that need to be re-downloaded:\", missing_files)\n",
    "    else:\n",
    "        print(\"All required files have been successfully downloaded.\")\n",
    "# Main function for managing downloads\n",
    "def initiate_downloads():\n",
    "    driver = setup_browser(download_directory)\n",
    "    try:\n",
    "        # Download each file URL\n",
    "        for category, links in urls.items():\n",
    "            for url in links:\n",
    "                driver.get(url)  # Open URL in Chrome for visibility\n",
    "                download_with_progress(url, download_directory)\n",
    "                time.sleep(1)\n",
    "    finally:\n",
    "        driver.quit()  # Close Chrome WebDriver\n",
    "        verify_downloads()\n",
    "# Run the download process\n",
    "if __name__ == \"__main__\":\n",
    "    initiate_downloads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error initializing log: [WinError 32] The process cannot access the file because it is being used by another process: 'E:\\\\SentinelNO2-Detection\\\\Geo_Data_Downloading\\\\data\\\\file_log.txt'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import logging\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "from folium.plugins import HeatMap, MarkerCluster\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure paths and logging\n",
    "data_dir = r\"E:\\SentinelNO2-Detection\\Geo_Data_Downloading\\data\"\n",
    "log_file_path = os.path.join(data_dir, \"file_log.txt\")\n",
    "\n",
    "logging.basicConfig(filename=log_file_path, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Initialize log file\n",
    "def initialize_log():\n",
    "    \"\"\"Initialize the log file.\"\"\"\n",
    "    try:\n",
    "        if os.path.exists(log_file_path):\n",
    "            os.remove(log_file_path)\n",
    "        logging.info(\"Log initialized. Starting directory scan and file processing...\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing log: {e}\")\n",
    "\n",
    "# Scan and extract files\n",
    "def scan_and_extract_files():\n",
    "    \"\"\"Scan the directory and extract ZIP files if needed.\"\"\"\n",
    "    files_in_directory = os.listdir(data_dir)\n",
    "    used_files, unused_files = [], []\n",
    "    \n",
    "    for file in files_in_directory:\n",
    "        file_path = os.path.join(data_dir, file)\n",
    "        \n",
    "        # Extract ZIP files\n",
    "        try:\n",
    "            if zipfile.is_zipfile(file_path):\n",
    "                with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "                    if not all(os.path.exists(os.path.join(data_dir, name)) for name in zip_ref.namelist()):\n",
    "                        zip_ref.extractall(data_dir)\n",
    "                        logging.info(f\"Extracted ZIP file: {file}\")\n",
    "                    else:\n",
    "                        logging.info(f\"ZIP file already extracted: {file}\")\n",
    "        \n",
    "            # Identify usable files\n",
    "            if file.endswith(('.pickle', '.shp', '.geojson', '.csv')):\n",
    "                used_files.append(file)\n",
    "            else:\n",
    "                unused_files.append(file)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to process file {file}: {e}\")\n",
    "    \n",
    "    logging.info(f\"Used files: {used_files}\")\n",
    "    logging.info(f\"Unused files: {unused_files}\")\n",
    "    return used_files\n",
    "\n",
    "# Load geospatial data\n",
    "def load_geospatial_data(files):\n",
    "    \"\"\"Load geospatial data from supported file types into GeoDataFrames.\"\"\"\n",
    "    geodataframes = []\n",
    "    for file in files:\n",
    "        file_path = os.path.join(data_dir, file)\n",
    "        try:\n",
    "            if file.endswith('.pickle'):\n",
    "                gdf = gpd.GeoDataFrame(pd.read_pickle(file_path))\n",
    "            elif file.endswith(('.geojson', '.shp')):\n",
    "                gdf = gpd.read_file(file_path)\n",
    "            elif file.endswith('.csv'):\n",
    "                df = pd.read_csv(file_path)\n",
    "                if 'longitude' in df.columns and 'latitude' in df.columns:\n",
    "                    gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.longitude, df.latitude))\n",
    "                else:\n",
    "                    logging.warning(f\"CSV file {file} lacks latitude/longitude columns.\")\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "            geodataframes.append(gdf)\n",
    "            logging.info(f\"Loaded file: {file}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading file {file}: {e}\")\n",
    "    return geodataframes\n",
    "\n",
    "# Advanced analysis\n",
    "def advanced_analysis(gdfs):\n",
    "    \"\"\"Perform advanced analysis including spatial joins, heatmaps, clustering, and statistics.\"\"\"\n",
    "    for i, gdf in enumerate(gdfs):\n",
    "        try:\n",
    "            # Spatial join with previous GDFs if available\n",
    "            if i > 0:\n",
    "                gdfs[0] = gpd.sjoin(gdfs[0], gdf, how=\"inner\", op=\"intersects\")\n",
    "                logging.info(\"Spatial join completed.\")\n",
    "            \n",
    "            # Calculate area and centroid\n",
    "            gdf[\"centroid\"] = gdf.centroid\n",
    "            gdf[\"area\"] = gdf.geometry.area\n",
    "            logging.info(\"Centroids and areas calculated.\")\n",
    "            \n",
    "            # Generate density heatmap\n",
    "            m = folium.Map(location=[20.5937, 78.9629], zoom_start=5)\n",
    "            heat_data = [[point.xy[1][0], point.xy[0][0]] for point in gdf[\"centroid\"]]\n",
    "            HeatMap(heat_data).add_to(m)\n",
    "            m.save(os.path.join(data_dir, f\"heatmap_{i}.html\"))\n",
    "            logging.info(\"Density heatmap saved.\")\n",
    "            \n",
    "            # Clustering and K-Means Clustering\n",
    "            marker_cluster = MarkerCluster()\n",
    "            for idx, row in gdf.iterrows():\n",
    "                folium.Marker(location=[row[\"centroid\"].y, row[\"centroid\"].x]).add_to(marker_cluster)\n",
    "            marker_cluster.add_to(m)\n",
    "            m.save(os.path.join(data_dir, f\"cluster_map_{i}.html\"))\n",
    "            logging.info(\"Cluster map saved.\")\n",
    "            \n",
    "            # Perform K-Means clustering if sufficient data points exist\n",
    "            if len(gdf) >= 3:\n",
    "                coords = np.array([[point.xy[0][0], point.xy[1][0]] for point in gdf[\"centroid\"]])\n",
    "                kmeans = KMeans(n_clusters=3).fit(coords)\n",
    "                gdf['cluster'] = kmeans.labels_\n",
    "                logging.info(\"K-Means clustering completed.\")\n",
    "            \n",
    "            # Zonal Statistics\n",
    "            if \"zone_column\" in gdf.columns:\n",
    "                zonal_stats = gdf.groupby(\"zone_column\").agg({'area': 'sum'}).rename(columns={'area': 'total_area'})\n",
    "                logging.info(f\"Zonal statistics computed: {zonal_stats}\")\n",
    "\n",
    "            # Spatial Autocorrelation (Moran's I)\n",
    "            try:\n",
    "                morans_i = stats.moran(gdf[\"area\"], gdf[\"centroid\"])\n",
    "                logging.info(f\"Spatial Autocorrelation (Moran's I): {morans_i}\")\n",
    "            except Exception as e:\n",
    "                logging.warning(\"Moran‚Äôs I calculation failed: \" + str(e))\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in advanced analysis for GDF {i}: {e}\")\n",
    "\n",
    "# Save visualizations and reports\n",
    "def save_visualizations_and_reports(gdfs):\n",
    "    \"\"\"Generate and save visualizations, including geospatial plots and interactive maps.\"\"\"\n",
    "    for i, gdf in enumerate(gdfs):\n",
    "        try:\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            gdf.plot(marker='o', color='red', markersize=5)\n",
    "            plt.title(\"Geospatial Data Plot\")\n",
    "            plt.xlabel(\"Longitude\")\n",
    "            plt.ylabel(\"Latitude\")\n",
    "            plt.savefig(os.path.join(data_dir, f\"plot_{i}.png\"))\n",
    "            logging.info(f\"Geospatial plot for GDF {i} saved.\")\n",
    "            \n",
    "            # Interactive Map with Layer Control\n",
    "            m = folium.Map(location=[20.5937, 78.9629], zoom_start=5)\n",
    "            for _, row in gdf.iterrows():\n",
    "                folium.CircleMarker(\n",
    "                    location=[row[\"centroid\"].y, row[\"centroid\"].x],\n",
    "                    radius=5,\n",
    "                    popup=row.get(\"name\", \"Location\"),\n",
    "                    color=\"blue\"\n",
    "                ).add_to(m)\n",
    "            folium.LayerControl().add_to(m)\n",
    "            m.save(os.path.join(data_dir, f\"interactive_map_{i}.html\"))\n",
    "            logging.info(\"Interactive map with layer control saved.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error saving visualizations for GDF {i}: {e}\")\n",
    "\n",
    "# Time series analysis\n",
    "def time_series_analysis(gdfs):\n",
    "    \"\"\"Analyze data over time if temporal data is available.\"\"\"\n",
    "    for gdf in gdfs:\n",
    "        if \"timestamp\" in gdf.columns:\n",
    "            try:\n",
    "                gdf[\"timestamp\"] = pd.to_datetime(gdf[\"timestamp\"])\n",
    "                time_series = gdf.set_index(\"timestamp\").resample(\"M\").mean()\n",
    "                time_series.plot(title=\"Temporal Trends\", xlabel=\"Time\", ylabel=\"Value\")\n",
    "                plt.savefig(os.path.join(data_dir, \"time_series_analysis.png\"))\n",
    "                logging.info(\"Time series analysis plot saved.\")\n",
    "            except Exception as e:\n",
    "                logging.error(\"Error in time series analysis: \" + str(e))\n",
    "\n",
    "# Cleanup log file if empty\n",
    "def cleanup_log():\n",
    "    try:\n",
    "        if os.path.exists(log_file_path) and os.path.getsize(log_file_path) == 0:\n",
    "            os.remove(log_file_path)\n",
    "            logging.info(\"Log file deleted as no issues were found.\")\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error cleaning up log file: \" + str(e))\n",
    "\n",
    "# Main execution\n",
    "initialize_log()\n",
    "files = scan_and_extract_files()\n",
    "gdfs = load_geospatial_data(files)\n",
    "advanced_analysis(gdfs)\n",
    "save_visualizations_and_reports(gdfs)\n",
    "time_series_analysis(gdfs)\n",
    "cleanup_log()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
