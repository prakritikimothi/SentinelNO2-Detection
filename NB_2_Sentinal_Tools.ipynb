{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: #4682B4;\">S5P NO2 Toolkit</span>\n",
    "### <span style=\"color: #DC143C;\">Streamlined Python functions for advanced satellite data management.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_31488\\2666735863.py:3: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ pandas is already installed.\n",
      "‚¨ÜÔ∏è Successfully upgraded pandas\n",
      "‚úÖ geopandas is already installed.\n",
      "‚¨ÜÔ∏è Successfully upgraded geopandas\n",
      "‚úÖ netCDF4 is already installed.\n",
      "‚¨ÜÔ∏è Successfully upgraded netCDF4\n",
      "‚úÖ numpy is already installed.\n",
      "‚¨ÜÔ∏è Successfully upgraded numpy\n",
      "‚úÖ matplotlib is already installed.\n",
      "‚¨ÜÔ∏è Successfully upgraded matplotlib\n",
      "‚úÖ requests is already installed.\n",
      "‚¨ÜÔ∏è Successfully upgraded requests\n",
      "‚úÖ xmltodict is already installed.\n",
      "‚¨ÜÔ∏è Successfully upgraded xmltodict\n",
      "‚úÖ shapely is already installed.\n",
      "‚¨ÜÔ∏è Successfully upgraded shapely\n",
      "üåü All libraries are imported successfully!\n",
      "‚úÖ All libraries are checked, installed, upgraded, and imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import pkg_resources\n",
    "import importlib\n",
    "# List of libraries to check/install/upgrade\n",
    "libraries = [\n",
    "    'pandas',\n",
    "    'geopandas',\n",
    "    'netCDF4',\n",
    "    'numpy',\n",
    "    'matplotlib',\n",
    "    'requests',\n",
    "    'xmltodict',\n",
    "    'shapely'\n",
    "]\n",
    "# Libraries that aren't directly installable via pip\n",
    "non_installable_libraries = [\n",
    "    'calendar',\n",
    "    'datetime',\n",
    "    're',\n",
    "    'socket',\n",
    "    'subprocess',\n",
    "    'mpl_toolkits.axes_grid1',\n",
    "    'urllib'\n",
    "]\n",
    "# Function to install a package\n",
    "def install_package(package):\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"üéâ Successfully installed {package}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚ùå Error installing {package}: {e}\")\n",
    "# Function to upgrade a package\n",
    "def upgrade_package(package):\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", package])\n",
    "        print(f\"‚¨ÜÔ∏è Successfully upgraded {package}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚ùå Error upgrading {package}: {e}\")\n",
    "# Function to check if a package is installed and install/upgrade it\n",
    "def check_and_install(library):\n",
    "    if library in non_installable_libraries:\n",
    "        print(f\"üîπ {library} is part of the Python standard library or not installable via pip.\")\n",
    "        return\n",
    "    try:\n",
    "        pkg_resources.get_distribution(library)\n",
    "        print(f\"‚úÖ {library} is already installed.\")\n",
    "        upgrade_package(library)\n",
    "    except pkg_resources.DistributionNotFound:\n",
    "        print(f\"üöÄ {library} is not installed. Installing now...\")\n",
    "        install_package(library)\n",
    "# Check and install/upgrade packages\n",
    "for library in libraries:\n",
    "    check_and_install(library)\n",
    "# Import all the necessary libraries\n",
    "def import_libraries():\n",
    "    globals().update(locals())\n",
    "    import pandas as pd\n",
    "    import geopandas as gpd\n",
    "    from os import listdir, rename, path, remove, mkdir\n",
    "    from os.path import isfile, join, getsize, exists\n",
    "    from netCDF4 import Dataset\n",
    "    import time\n",
    "    import numpy as np\n",
    "    import calendar\n",
    "    import datetime as dt\n",
    "    import re\n",
    "    from socket import timeout\n",
    "    from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "    import matplotlib.pyplot as plt\n",
    "    import urllib\n",
    "    import requests, json\n",
    "    from requests.auth import HTTPBasicAuth\n",
    "    import xmltodict\n",
    "    from shapely import wkt\n",
    "# Try to import all libraries and handle any potential import errors\n",
    "try:\n",
    "    import_libraries()\n",
    "    print(\"üåü All libraries are imported successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Error importing libraries: {e}\")\n",
    "print(\"‚úÖ All libraries are checked, installed, upgraded, and imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_project(project_name='default'):\n",
    "    \"\"\"    \n",
    "    Description\n",
    "    -----------\n",
    "    This function checks if the specified subfolder exists. If not, it attempts to create the subfolder.\n",
    "    If the subfolder is successfully created, it returns the subfolder's name. Otherwise, it returns an\n",
    "    empty string indicating failure. If the subfolder already exists, it returns the subfolder's name.\n",
    "    \"\"\"\n",
    "    # Check if the directory already exists\n",
    "    if not os.path.exists(project_name):\n",
    "        try:\n",
    "            # Attempt to create the directory\n",
    "            os.mkdir(project_name)\n",
    "        except OSError as e:\n",
    "            # Handle any errors during the directory creation\n",
    "            print(f\"‚ùå Creation of the directory {project_name} failed: {e}\")\n",
    "            return ''\n",
    "        else:\n",
    "            # Successfully created the directory\n",
    "            print(f\"üéâ Successfully created the directory {project_name}.\")\n",
    "            return project_name\n",
    "    else:\n",
    "        # Directory already exists\n",
    "        print(f\"‚ÑπÔ∏è Directory {project_name} already exists.\")\n",
    "        return project_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_place_boundingbox(place_gdf, buffer_distance):\n",
    "    \"\"\"\n",
    "    Determine the bounding box for a given GeoDataFrame representing a place.\n",
    "    Parameters\n",
    "    ----------\n",
    "    place_gdf : GeoDataFrame\n",
    "        A GeoDataFrame containing the geographic data of the place. This should be a Level 0 polygon from GADM.\n",
    "    buffer_distance : int\n",
    "        The distance in miles to extend the boundaries of the place.\n",
    "    Returns\n",
    "    -------\n",
    "    GeoDataFrame\n",
    "        A new GeoDataFrame containing the bounding box around the place.\n",
    "    Description\n",
    "    -----------\n",
    "    This function takes the geometric shape of a place, expands its boundaries by a specified distance,\n",
    "    and returns a rectangle that fully contains the expanded shape.\n",
    "    \"\"\"\n",
    "    # Create the bounding box with the specified buffer\n",
    "    expanded_geometry = place_gdf['geometry'].buffer(buffer_distance).envelope\n",
    "    bounding_box_gdf = gpd.GeoDataFrame(geometry=expanded_geometry, crs=place_gdf.crs).reset_index()\n",
    "    return bounding_box_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_swath_set(swath_set_gdf, place_gdf):\n",
    "    \"\"\"\n",
    "    Filter swaths based on the place constraint.\n",
    "    Parameters\n",
    "    ----------\n",
    "    swath_set_gdf : GeoDataFrame\n",
    "        GeoDataFrame containing swath geometries.\n",
    "    place_gdf : GeoDataFrame\n",
    "        GeoDataFrame of the place to filter swaths for.\n",
    "    Returns\n",
    "    -------\n",
    "    GeoDataFrame\n",
    "        Subset of swath_set_gdf containing geometries that cover place_gdf.\n",
    "    \"\"\"\n",
    "    filtered_gdf = gpd.sjoin(swath_set_gdf, place_gdf, how='right', op='contains').reset_index()\n",
    "    filtered_gdf = filtered_gdf.drop(columns=['level_0','index_left','index'])\n",
    "    return filtered_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geometry_to_wkt(place_gdf):\n",
    "    \"\"\"\n",
    "    Convert GeoDataFrame geometry to Well-Known Text (WKT) format.\n",
    "    Description\n",
    "    -----------\n",
    "    This function converts the geometry of a given place GeoDataFrame into a Well-Known Text (WKT) format.\n",
    "    This format is required for the Sentinel 5P Data Access hub to constrain the polygon filter, allowing\n",
    "    for the retrieval of a smaller number of satellite image swaths.\n",
    "    \"\"\"\n",
    "    # Get the geometry's convex hull and simplify it\n",
    "    simplified_geometry = place_gdf.reset_index()['geometry'].convex_hull.simplify(tolerance=0.05)\n",
    "    # Convert the simplified geometry to WKT\n",
    "    wkt_string = wkt.dumps(simplified_geometry[0])\n",
    "    return wkt_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_from_week(week_string='2019-W01'):\n",
    "    \"\"\"\n",
    "    Convert a week string to a datetime object.\n",
    "    \"\"\"\n",
    "    return dt.datetime.strptime(week_string + '-1', \"%Y-W%W-%w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_days(start, num_days=1):\n",
    "    \"\"\"\n",
    "    Add a number of days to a start date and return both dates.\n",
    "    \"\"\"\n",
    "    end = start + dt.timedelta(days=num_days)\n",
    "    return [start.strftime(\"%Y-%m-%d\"), end.strftime(\"%Y-%m-%d\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nc_to_df(ncfile):\n",
    "    \"\"\"\n",
    "    Convert TROPOMI NO2 NetCDF file to DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        file = Dataset(ncfile, 'r')\n",
    "    except OSError:\n",
    "        print('Cannot open', ncfile)\n",
    "        return pd.DataFrame()\n",
    "    if 'NO2___' not in ncfile or 'S5P' not in ncfile:\n",
    "        raise NameError('Not a TROPOMI NO2 file name.')\n",
    "    grp = 'PRODUCT'\n",
    "    lat = file[grp].variables['latitude'][0][:][:]\n",
    "    lon = file[grp].variables['longitude'][0][:][:]\n",
    "    data = file[grp].variables['nitrogendioxide_tropospheric_column']\n",
    "    fv = data._FillValue\n",
    "    scan_time = file[grp].variables['time_utc']\n",
    "    timestamps = [dt.datetime.strptime(t.split('.')[0], '%Y-%m-%dT%H:%M:%S').timestamp() for t in scan_time[0]]\n",
    "    df = pd.DataFrame({\n",
    "        'UnixTimestamp': np.repeat(timestamps, lat.shape[1]),\n",
    "        'DateTime': pd.to_datetime(np.repeat(timestamps, lat.shape[1]), unit='s')\n",
    "    })\n",
    "    df[['Date', 'Time']] = df['DateTime'].astype(str).str.split(' ', expand=True)\n",
    "    for var in file[grp].variables.keys():\n",
    "        sds = file[grp].variables[var]\n",
    "        if len(sds.shape) == 3:\n",
    "            scale = sds.scale_factor if 'qa' in var else 1.0\n",
    "            data = np.where(sds[:].ravel() == fv, np.nan, sds[:].ravel() * scale)\n",
    "            df[var] = data\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polygon_filter(input_df, filter_gdf):\n",
    "    \"\"\"\n",
    "    Remove records from NO2 DataFrame outside filter polygons.\n",
    "    \"\"\"\n",
    "    print(\"Ensure you've created the spatial index for filter_gdf with filter_gdf.sindex.\")\n",
    "    tic = time.perf_counter()\n",
    "    # Convert input_df to GeoDataFrame with same CRS as filter_gdf\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        input_df, \n",
    "        geometry=gpd.points_from_xy(input_df.longitude, input_df.latitude),\n",
    "        crs=filter_gdf.crs\n",
    "    )\n",
    "    print(f\"Original NO2 DataFrame length: {len(gdf)}\")\n",
    "    # Perform spatial join to filter data\n",
    "    filtered_gdf = gpd.sjoin(gdf, filter_gdf, how='inner', op='intersects')\n",
    "    filtered_gdf = filtered_gdf.drop(columns=['index_right'])\n",
    "    print(f\"Filtered NO2 GeoDataFrame length: {len(filtered_gdf)}\")\n",
    "    toc = time.perf_counter()\n",
    "    print(f\"Processed NO2 DataFrame sjoin in {str((toc - tic) / 60)} minutes\")\n",
    "    return filtered_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename_from_cd(cd):\n",
    "    \"\"\"\n",
    "    Extract filename from content-disposition (cd) header.\n",
    "    \"\"\"\n",
    "    if not cd:\n",
    "        return None\n",
    "    fname = re.findall('filename=(.+)', cd)\n",
    "    if not fname:\n",
    "        return None\n",
    "    return fname[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_nc_file(url, auth, savedir, logging=False, refresh=False, chunk_size=1024*1024):\n",
    "    \"\"\"\n",
    "    Downloads NetCDF files from a URL. Takes URL, user auth, save directory, logging, refresh, and chunk size as parameters. Returns the filename.\n",
    "    \"\"\"\n",
    "    user, password = auth['user'], auth['password']\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',\n",
    "    }\n",
    "    try:\n",
    "        tic = time.perf_counter()\n",
    "        response = requests.get(url, auth=(user, password), stream=True, headers=headers)\n",
    "        content_disposition = response.headers.get('content-disposition')\n",
    "        filename = content_disposition.split('filename=')[-1].replace('\"', '') if content_disposition else 'downloaded_file.nc'\n",
    "        file_path = os.path.join(savedir, filename)\n",
    "        if os.path.exists(file_path) and not refresh:\n",
    "            if os.path.getsize(file_path) > 0:\n",
    "                return filename\n",
    "        with open(file_path, 'wb') as f:\n",
    "            for data in response.iter_content(chunk_size=chunk_size):\n",
    "                f.write(data)\n",
    "        if logging:\n",
    "            with open(os.path.join(savedir, 'nc.log'), 'a+') as l:\n",
    "                l.seek(0)\n",
    "                if l.read(100):\n",
    "                    l.write(\"\\n\")\n",
    "                l.write(filename)\n",
    "        toc = time.perf_counter()\n",
    "        print(f'Success: Saved {filename} to {savedir}.')\n",
    "        print(f'Download time: {toc-tic:.2f} seconds')\n",
    "        delay = np.random.choice([7, 4, 6, 2, 10, 15, 19, 23])\n",
    "        print(f'Delaying for {delay} seconds...')\n",
    "        time.sleep(delay)\n",
    "        return filename\n",
    "    except Exception as e:\n",
    "        print('Something went wrong:', e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harpconvert(input_filename, input_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Converts TROPOMI NO2 NetCDF to HDF5 (L3 Analysis).\n",
    "    Args:\n",
    "        input_filename (str): Name of the input NetCDF file.\n",
    "        input_dir (str): Directory of the input file.\n",
    "        output_dir (str): Directory to save the HDF5 file.\n",
    "    Returns:\n",
    "        dict: Contains filename, filesize, elapsed time, stdout, stderr.\n",
    "    \"\"\"\n",
    "    tic = time.perf_counter()\n",
    "    output_filename = input_filename.replace('.nc', '.h5')\n",
    "    input_path = os.path.join(input_dir, input_filename)\n",
    "    output_path = os.path.join(output_dir, output_filename)\n",
    "    cmd = (\n",
    "        f\"harpconvert --format hdf5 --hdf5-compression 9 \"\n",
    "        f\"-a 'tropospheric_NO2_column_number_density_validity>50;derive(datetime_stop {{time}})' \"\n",
    "        f\"{input_path} {output_path}\"\n",
    "    )\n",
    "    process = subprocess.Popen(['bash', '-c', cmd], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    stdout, stderr = process.communicate()\n",
    "    elapsed_time = time.perf_counter() - tic\n",
    "    if not os.path.exists(output_path):\n",
    "        raise FileNotFoundError(f\"Output file {output_filename} was not created.\")\n",
    "    checksum = subprocess.check_output(['sha256sum', output_path]).split()[0].decode()\n",
    "    status_dict = {\n",
    "        'input_filename': input_filename,\n",
    "        'output_filesize': f\"{os.path.getsize(output_path):,} bytes\",\n",
    "        'elapsed_time': f\"{elapsed_time:.2f} seconds\",\n",
    "        'stdout': stdout.decode(),\n",
    "        'stderr': stderr.decode(),\n",
    "        'checksum': checksum,\n",
    "    }\n",
    "    return status_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_assemble_filtered_pickles(filtered_dir):\n",
    "    \"\"\"\n",
    "    Assembles DataFrames from pickle files in a directory.\n",
    "    Args:\n",
    "        filtered_dir (str): Directory containing the pickle files.\n",
    "    Returns:\n",
    "        DataFrame: Concatenated DataFrame of all pickle files.\n",
    "    \"\"\"\n",
    "    tic = time.perf_counter()\n",
    "    pickle_files = [f for f in os.listdir(filtered_dir) if os.path.isfile(os.path.join(filtered_dir, f))]\n",
    "    full_df = pd.DataFrame()\n",
    "    for pickle_file in pickle_files:\n",
    "        print(pickle_file)\n",
    "        df = pd.read_pickle(os.path.join(filtered_dir, pickle_file))\n",
    "        full_df = pd.concat([df, full_df], axis=0)\n",
    "    elapsed_time = (time.perf_counter() - tic) / 60\n",
    "    print(f'Assembly time: {elapsed_time:.2f} minutes')\n",
    "    output_filename = os.path.join(filtered_dir, 'assembled_dataframe.pkl')\n",
    "    full_df.to_pickle(output_filename)\n",
    "    print(f'Saved assembled DataFrame to {output_filename}')\n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_maps(iso3, filter_gdf, filelist, colormap, sensing_date):\n",
    "    \"\"\"\n",
    "    Plots TROPOMI NO2 data on maps.\n",
    "    Args:\n",
    "        iso3 (str): 3-letter ISO country code.\n",
    "        filter_gdf (GeoDataFrame): Filtered GeoDataFrame.\n",
    "        filelist (list): List of pickle files.\n",
    "        colormap (str): Colormap for the plot.\n",
    "        sensing_date (str): Date of sensing.\n",
    "    Returns:\n",
    "        Matplotlib figure object.\n",
    "    \"\"\"\n",
    "    crs = filter_gdf.crs\n",
    "    country_gdf = filter_gdf[filter_gdf['iso3'] == iso3]\n",
    "    country_name = country_gdf['name'].unique()[0]\n",
    "    gdf_sjoin_list = []\n",
    "    for file in filelist:\n",
    "        gdf_sjoin = pd.read_pickle(file)\n",
    "        gdf_sjoin = gdf_sjoin.set_geometry('geometry').to_crs(crs)\n",
    "        gdf_countries_sjoin = gpd.sjoin(gdf_sjoin, country_gdf, how='inner', op='intersects')\n",
    "        if not gdf_countries_sjoin.empty:\n",
    "            gdf_sjoin_list.append(gdf_countries_sjoin)\n",
    "    print(f'Using {len(gdf_sjoin_list)} swaths.')\n",
    "    def get_column_range(gdf_list, column):\n",
    "        return min(gdf[column].min() for gdf in gdf_list), max(gdf[column].max() for gdf in gdf_list)\n",
    "    vmin_qa, vmax_qa = get_column_range(gdf_sjoin_list, 'qa_value')\n",
    "    vmin_no2, vmax_no2 = get_column_range(gdf_sjoin_list, 'nitrogendioxide_tropospheric_column')\n",
    "    def plot_with_colorbar(ax, gdf_list, column, vmin, vmax, title):\n",
    "        for gdf in gdf_list:\n",
    "            gdf.plot(ax=ax, column=column, cmap=plt.get_cmap(colormap), vmin=vmin, vmax=vmax, alpha=0.9)\n",
    "        country_gdf.plot(ax=ax, color='None', edgecolor='black', alpha=0.5)\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes(\"right\", size=\"3%\", pad=0.1)\n",
    "        sm = plt.cm.ScalarMappable(cmap=colormap, norm=plt.Normalize(vmin=vmin, vmax=vmax))\n",
    "        sm._A = []\n",
    "        ax.get_figure().colorbar(sm, cax=cax)\n",
    "        ax.set_title(title)\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(8, 12), constrained_layout=True, sharex=True, sharey=True)\n",
    "    plot_with_colorbar(axs[0], gdf_sjoin_list, 'qa_value', vmin_qa, vmax_qa, f'Tropospheric NO2, QA Value ({country_name}, {sensing_date})')\n",
    "    plot_with_colorbar(axs[1], gdf_sjoin_list, 'nitrogendioxide_tropospheric_column_precision_kernel', vmin_no2, vmax_no2, f'Tropospheric NO2, Tropospheric Column, moles/m¬≤ ({country_name}, {sensing_date})')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentinel_api_query(query_dict, silentmode=False):\n",
    "    \"\"\"\n",
    "    Queries Sentinel-5P data and returns results as a GeoDataFrame.\n",
    "    Args:\n",
    "        query_dict (dict): API query variables.\n",
    "        silentmode (bool): Suppress print statements if True.\n",
    "    Returns:\n",
    "        GeoDataFrame: GeoDataFrame containing the query results.\n",
    "    \"\"\"\n",
    "    delay = np.random.choice([7, 4, 6, 2, 10, 15, 19, 23])\n",
    "    if not silentmode:\n",
    "        print(f'Delaying for {delay} seconds...')\n",
    "    time.sleep(delay)\n",
    "    # Unpack query_dict\n",
    "    polygon = query_dict['polygon']\n",
    "    startDate = query_dict['startDate']\n",
    "    endDate = query_dict['endDate']\n",
    "    platformName = query_dict['platformName']\n",
    "    productType = query_dict['productType']\n",
    "    processingLevel = query_dict['processingLevel']\n",
    "    processingMode = query_dict['processingMode']\n",
    "    dhus_url = query_dict['dhus_url']\n",
    "    startPage = query_dict['startPage']\n",
    "    numRows = query_dict['numRows']\n",
    "    username = query_dict['username']\n",
    "    password = query_dict['password']\n",
    "    # Construct query string for API\n",
    "    query = (\n",
    "        f'( footprint:\"Intersects({polygon})\") AND '\n",
    "        f'( beginPosition:[{startDate}T00:00:00.000Z TO {endDate}T23:59:59.999Z] AND '\n",
    "        f'endPosition:[{startDate}T00:00:00.000Z TO {endDate}T23:59:59.999Z] ) AND '\n",
    "        f'( (platformname:{platformName} AND producttype:{productType} '\n",
    "        f'AND processinglevel:{processingLevel} AND processingmode:{processingMode}))'\n",
    "    )\n",
    "    quoted = urllib.parse.quote_plus(query)\n",
    "    if not silentmode:\n",
    "        print(f'query: {query}')\n",
    "        print(f'quoted: {quoted}')\n",
    "    # Send query to API and get response\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "    response = requests.get(f'{dhus_url}dhus/search?q={quoted}&start={startPage}&rows={numRows}',\n",
    "                            auth=HTTPBasicAuth(username, password), headers=headers)\n",
    "    if not silentmode:\n",
    "        print(f'headers: {response.headers}')\n",
    "        print(f'cookie: {response.headers.get(\"Set-Cookie\", \"\")}')\n",
    "    # Convert XML response to dictionary\n",
    "    my_dict = xmltodict.parse(response.text)\n",
    "    results = int(my_dict['feed']['opensearch:totalResults'])\n",
    "    if results > numRows and not silentmode:\n",
    "        print(f'WARNING: Returned results {results} exceeds requested number of rows ({numRows}).')\n",
    "    # Store dictionary items in Pandas DataFrame\n",
    "    records = []\n",
    "    for item in my_dict['feed']['entry']:\n",
    "        gmldict = xmltodict.parse(item['str'][1]['#text'])\n",
    "        crs = gmldict['gml:Polygon']['@srsName'].split('#')\n",
    "        record = {\n",
    "            'ingestiondate': item['date'][0]['#text'],\n",
    "            'beginposition': item['date'][1]['#text'],\n",
    "            'endposition': item['date'][2]['#text'],\n",
    "            'orbitnumber': item['int']['#text'],\n",
    "            'filename': item['str'][0]['#text'],\n",
    "            'crs': f'epsg:{crs[1]}',\n",
    "            'format': item['str'][2]['#text'],\n",
    "            'identifier': item['str'][3]['#text'],\n",
    "            'instrumentname': item['str'][4]['#text'],\n",
    "            'instrumentshortname': item['str'][5]['#text'],\n",
    "            'footprint': item['str'][6]['#text'],\n",
    "            'mission': item['str'][7]['#text'],\n",
    "            'platformname': item['str'][8]['#text'],\n",
    "            'platformserialidentifier': item['str'][9]['#text'],\n",
    "            'platformshortname': item['str'][10]['#text'],\n",
    "            'processinglevel': item['str'][11]['#text'],\n",
    "            'processingmode': item['str'][12]['#text'],\n",
    "            'processingmodeabbreviation': item['str'][13]['#text'],\n",
    "            'processorversion': item['str'][14]['#text'],\n",
    "            'producttype': item['str'][15]['#text'],\n",
    "            'producttypedescription': item['str'][16]['#text'],\n",
    "            'revisionnumber': item['str'][17]['#text'],\n",
    "            'size': item['str'][18]['#text'],\n",
    "            'uuid': item['str'][19]['#text'],\n",
    "            'downloadurl': item['link'][0]['@href']\n",
    "        }\n",
    "        records.append(record)\n",
    "        if not silentmode:\n",
    "            print(record)\n",
    "    # Convert DataFrame to GeoDataFrame and return\n",
    "    study_df = pd.DataFrame(records)\n",
    "    study_df['geometry'] = study_df['footprint'].apply(wkt.loads)\n",
    "    study_df['beginposition'] = pd.to_datetime(study_df['beginposition'].str.replace('T', ' '))\n",
    "    study_df['endposition'] = pd.to_datetime(study_df['endposition'].str.replace('T', ' '))\n",
    "    study_df['startdate'] = study_df['beginposition'].dt.strftime('%Y-%m-%d')\n",
    "    study_df['enddate'] = study_df['endposition'].dt.strftime('%Y-%m-%d')\n",
    "    study_gdf = gpd.GeoDataFrame(study_df, crs={'init': 'epsg:4326'}, geometry='geometry')\n",
    "    return study_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_color_gradients(gradient, cmap_category, cmap_list, save=False, output_dir='.'):\n",
    "    \"\"\"\n",
    "    Plots color gradients for a given list of colormaps.\n",
    "    Args:\n",
    "        gradient (ndarray): Gradient to display colormaps.\n",
    "        cmap_category (str): Category of the colormaps.\n",
    "        cmap_list (list): List of colormap names.\n",
    "        save (bool): Save the plot if True.\n",
    "        output_dir (str): Directory to save the plots.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(6, 0.4 * len(cmap_list)))\n",
    "    for i, cmap_name in enumerate(cmap_list):\n",
    "        ax.imshow(gradient, aspect='auto', cmap=plt.get_cmap(cmap_name))\n",
    "        ax.text(-0.01, 0.5 * (2 * i + 1), cmap_name, va='center', ha='right', fontsize=10, transform=ax.transAxes)\n",
    "    ax.set_axis_off()\n",
    "    plt.title(cmap_category, loc='left', fontsize=12)\n",
    "    if save:\n",
    "        plt.savefig(f\"{output_dir}/{cmap_category}.png\")\n",
    "    plt.show()\n",
    "def show_colormap(filter_category=None, save=False, output_dir='.', resolution=256):\n",
    "    \"\"\"\n",
    "    Displays available colormaps, with options to filter by category, save the plots, and set gradient resolution.\n",
    "    Args:\n",
    "        filter_category (str): Filter colormap categories (e.g., 'Sequential').\n",
    "        save (bool): Save the plots if True.\n",
    "        output_dir (str): Directory to save the plots.\n",
    "        resolution (int): Resolution of the gradient.\n",
    "    Returns:\n",
    "        bool: True if successful.\n",
    "    \"\"\"\n",
    "    cmaps = [\n",
    "        ('Perceptually Uniform Sequential', ['viridis', 'plasma', 'inferno', 'magma', 'cividis']),\n",
    "        ('Sequential', ['Greys', 'Purples', 'Blues', 'Greens', 'Oranges', 'Reds', 'YlOrBr', 'YlOrRd', 'OrRd', 'PuRd', 'RdPu', 'BuPu', 'GnBu', 'PuBu', 'YlGnBu', 'PuBuGn', 'BuGn', 'YlGn']),\n",
    "        ('Sequential (2)', ['binary', 'gist_yarg', 'gist_gray', 'gray', 'bone', 'pink', 'spring', 'summer', 'autumn', 'winter', 'cool', 'Wistia', 'hot', 'afmhot', 'gist_heat', 'copper']),\n",
    "        ('Diverging', ['PiYG', 'PRGn', 'BrBG', 'PuOr', 'RdGy', 'RdBu', 'RdYlBu', 'RdYlGn', 'Spectral', 'coolwarm', 'bwr', 'seismic']),\n",
    "        ('Cyclic', ['twilight', 'twilight_shifted', 'hsv']),\n",
    "        ('Qualitative', ['Pastel1', 'Pastel2', 'Paired', 'Accent', 'Dark2', 'Set1', 'Set2', 'Set3', 'tab10', 'tab20', 'tab20b', 'tab20c']),\n",
    "        ('Miscellaneous', ['flag', 'prism', 'ocean', 'gist_earth', 'terrain', 'gist_stern', 'gnuplot', 'gnuplot2', 'CMRmap', 'cubehelix', 'brg', 'gist_rainbow', 'rainbow', 'jet', 'nipy_spectral', 'gist_ncar'])\n",
    "    ]\n",
    "    gradient = np.linspace(0, 1, resolution)\n",
    "    gradient = np.vstack((gradient, gradient))\n",
    "    for cmap_category, cmap_list in cmaps:\n",
    "        if filter_category and cmap_category != filter_category:\n",
    "            continue\n",
    "        plot_color_gradients(gradient, cmap_category, cmap_list, save=save, output_dir=output_dir)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_color_gradients(gradient, cmap_category, cmap_list, save=False, output_dir='.', title=None, resolution=256):\n",
    "    \"\"\"\n",
    "    Plots color gradients for a given list of colormaps with options to save and customize the plot.\n",
    "    Args:\n",
    "        gradient (ndarray): Gradient to display colormaps.\n",
    "        cmap_category (str): Category of the colormaps.\n",
    "        cmap_list (list): List of colormap names.\n",
    "        save (bool): Save the plot if True.\n",
    "        output_dir (str): Directory to save the plots.\n",
    "        title (str): Custom title for the plot.\n",
    "        resolution (int): Resolution of the gradient.\n",
    "    \"\"\"\n",
    "    nrows = len(cmap_list)\n",
    "    figh = 0.35 + 0.15 + (nrows + (nrows-1)*0.1)*0.22\n",
    "    fig, axes = plt.subplots(nrows=nrows, figsize=(6.4, figh))\n",
    "    fig.subplots_adjust(top=1-.35/figh, bottom=.15/figh, left=0.2, right=0.99)\n",
    "    axes[0].set_title(title if title else cmap_category + ' colormaps', fontsize=14)\n",
    "    gradient = np.linspace(0, 1, resolution)\n",
    "    gradient = np.vstack((gradient, gradient))\n",
    "    for ax, name in zip(axes, cmap_list):\n",
    "        ax.imshow(gradient, aspect='auto', cmap=plt.get_cmap(name))\n",
    "        ax.text(-.01, .5, name, va='center', ha='right', fontsize=10, transform=ax.transAxes)\n",
    "    for ax in axes:\n",
    "        ax.set_axis_off()\n",
    "    if save:\n",
    "        plt.savefig(f\"{output_dir}/{cmap_category}.png\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
